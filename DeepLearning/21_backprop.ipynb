{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6f2da2e73155c155393b192321691115",
     "grade": false,
     "grade_id": "cell-87195ccb7e06731c",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Number of points for this notebook:</b> 2\n",
    "<br>\n",
    "<b>Deadline:</b> March 9, 2020 (Monday). 23:00\n",
    "</div>\n",
    "\n",
    "# Exercise 2.1. Backpropagation\n",
    "\n",
    "In this assignment, you will implement forward and backward computations in a multilayer perceptron (MLP) network using pure [`numpy`](https://numpy.org). This will help you understand:\n",
    "* The concept of backpropagation.\n",
    "* How to implement and test gradient computations.\n",
    "* How automatic differentiation is implemented in packages like [`PyTorch`](https://pytorch.org/docs/stable/index.html).\n",
    "\n",
    "**In this notebook, we will implement MLP blocks that process only a single training sample at a time.**\n",
    "\n",
    "**Foreward:**\n",
    "* Please avoid using global variables within the functions.\n",
    "* You need to brush up your knowledge of [`numpy`](https://numpy.org) to do this assignment.\n",
    "* Inserting cells does not cause problems to autograding. You can insert cells to test your implementation of the gradient computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c21e9d6cb2d76851a6eb4b2babfc8f17",
     "grade": false,
     "grade_id": "cell-cafdead5e95c3773",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "bb53da3ebcda7888a7eca8a2aceccfa2",
     "grade": false,
     "grade_id": "cell-7473a37e84b4a136",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "First, we illustrate gradient computations with a simple example. Consider a function:\n",
    "\n",
    "$$f(\\mathbf{x}) = \\mathbf{x}^T \\mathbf{x} = x_1^2 + x_2^2.$$\n",
    "\n",
    "Recall that gradient is a vector containing all partial derivatives of function $f$ w.r.t. all inputs\n",
    "($x_1$ and $x_2$ in our case). We want to compute the gradient of $f(\\mathbf{x})$ for any given input\n",
    "$\\mathbf{x}$.\n",
    "\n",
    "The value of the gradient computed analytically is:\n",
    "$$\\nabla f(\\mathbf{x}) = 2 \\mathbf{x}$$\n",
    "which for some (aribitrarily selected) input $\\mathbf{x} = \\begin{bmatrix}1\\\\2\\end{bmatrix}$ is\n",
    "$$\\nabla f(\\mathbf{x}) = 2 \\mathbf{x} = \\begin{bmatrix}2\\\\4\\end{bmatrix}.$$\n",
    "In this notebook, we call a vector computed in this way an *analytical gradient*.\n",
    "\n",
    "To test our impementation of gradient computations, we can compute the value of the gradient for \n",
    "the same input $\\mathbf{x} = \\begin{bmatrix}1\\\\2\\end{bmatrix}$ using [numerical differentiation](https://en.wikipedia.org/wiki/Numerical_differentiation):\n",
    "$$ \\nabla f(\\mathbf{x}) \\approx \\frac{f(\\mathbf{x} + \\epsilon) - f(\\mathbf{x} - \\epsilon)}{2\\epsilon}$$\n",
    "which, using $\\epsilon=0.5$, yields\n",
    "$$\\nabla f(\\mathbf{x}) =\n",
    "\\frac{1}{2 \\epsilon}\n",
    "\\begin{bmatrix}\n",
    "  (1+\\epsilon)^2 + 2^2 - ((1-\\epsilon)^2 + 2^2)\n",
    "\\\\\n",
    "  1^2 + (2+\\epsilon)^2 - (1^2 + (2-\\epsilon)^2)\n",
    "\\end{bmatrix}\n",
    "= \\begin{bmatrix}1.9999\\\\4.0000\\end{bmatrix}\n",
    "$$\n",
    "Note that the values of the numerical and analytical gradients  might _not exactly_ match to the decimal.\n",
    "\n",
    "The function below implements numerical computations of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "af6bb3a6e044fb4f6ece3778c6bf79f6",
     "grade": false,
     "grade_id": "cell-14c19df006e22022",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def numerical_gradient(fun, x, eps=1e-4):\n",
    "    \"\"\"Compute derivatives of a given function fun numerically.\n",
    "    \n",
    "    Args:\n",
    "      fun: A python function fun(x) which accepts a vector argument (one-dimensional numpy array)\n",
    "           and returns a vector output (one-dimensional numpy array).\n",
    "      x:   An input vector for which the numerical gradient should be computed.\n",
    "      eps: A scalar which defines the magnitude of perturbations applied to the inputs\n",
    "           (epsilon in the formula in the previous cell).\n",
    "\n",
    "    Returns:\n",
    "      gnum: A two-dimensional array in which an element in row i and column j is the partial derivative of the\n",
    "            i-th output of function fun wrt j-th input of function fun (computed numerically).\n",
    "    \"\"\"\n",
    "    assert x.ndim <= 1, \"Only vector inputs are supported\"\n",
    "    e = np.zeros_like(x)\n",
    "    f = fun(x)\n",
    "    assert f.ndim <= 1, \"Only vector outputs are supported\"\n",
    "    gnum = np.zeros((f.size, x.size))\n",
    "    for i in range(len(x)):\n",
    "        e[:] = 0\n",
    "        e[i] = 1\n",
    "        f1, f2 = fun(x + e*eps), fun(x - e * eps)\n",
    "        gnum[:, i] = (f1 - f2) / (2 * eps)\n",
    "    return gnum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6c17be491354fbaabdb36e4e7d2d5ef4",
     "grade": false,
     "grade_id": "cell-74c2c0ff1b83e1d0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "<a id='reshaping_trick'></a>\n",
    "\n",
    "**Reshaping trick**\n",
    "\n",
    "Note that function `numerical_gradient` defined above accepts functions `fun` that works only with *one-dimensional arrays* as inputs and outputs.\n",
    "\n",
    "Suppose we have function `fun(X)` which accepts a two-dimensional array `X` of shape `(n1, n2)` as input and produces a one-dimensional array `y` of shape `(ny,)` as output. We want to compute partial derivatives\n",
    "`d y[i] / d X[k,l]` for each output element `y[i]` and each element `X[k,l]` of the input matrix. We can to it in the following way.\n",
    "\n",
    "First, we define a function with one-dimensional inputs such that it can be passed to our `numerical_gradient`\n",
    "function. Function `fun2` reshapes a one-dimensional array passed to it and calls function `fun`:\n",
    "```\n",
    "fun2 = lambda A: fun(A.reshape(n1, n2))\n",
    "```\n",
    "\n",
    "Then we can call the `numerical_gradient` function:\n",
    "```\n",
    "A = np.random.randn(n1, n2)\n",
    "dA = numerical_gradient(fun2, A.flatten())\n",
    "```\n",
    "which will produce a two dimensional array of shape `(ny, n1*n2)` that will contain the required partial\n",
    "derivatives.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "f7eed0053c5a2cbae5b6755824bc7e69",
     "grade": false,
     "grade_id": "cell-cc9de3851f9d6d64",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 1. Gradient of the loss\n",
    "\n",
    "Let us first compute the gradient of a simple loss function wrt to its inputs. Consider the mean-squared error loss:\n",
    "$$\n",
    "c = \\frac{1}{n} \\sum_{i=1}^n (y_i - t_i)^2\n",
    "$$\n",
    "where $y_i$ are the elements of an input vector $\\mathbf{y}$ and $t_i$ are the elements of the target vector $\\mathbf{t}$.\n",
    "\n",
    "In the code below, we define a class that performs forward and backward computations of this loss function. The `backward` function should compute the gradient $\\frac{\\partial c}{\\partial \\mathbf{y}}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "9517c2edb7145ae90eb927e0fb3a73f9",
     "grade": false,
     "grade_id": "cell-32dc00fe264ae2cd",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MSELoss:\n",
    "    def forward(self, y, target):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          y (array):      Inputs of the loss function (can be, e.g., an output of a neural network),\n",
    "                           shape (n_features,).\n",
    "          target (array): Targets, shape (n_features,).\n",
    "        \"\"\"\n",
    "        self.diff = diff = y - target  # Keep this for backward computations\n",
    "        c = np.sum(np.square(diff)) / diff.size\n",
    "        return c\n",
    "\n",
    "    def backward(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "          dy (array): Gradient of the MSE loss wrt the inputs, shape (n_features,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'diff'), \"Need to call forward() first\"\n",
    "\n",
    "        dy=self.diff.__mul__((2/self.diff.size))\n",
    "        return dy\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d720f241dc1172a23fb23973ef9cfec3",
     "grade": false,
     "grade_id": "cell-f207af4f073b6735",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MSELoss_shapes():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "    dy = loss.backward()  # Do backward computations\n",
    "    assert dy.shape == y.shape, f\"Bad dy.shape: {dy.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_shapes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bd28181bafd56ddde73e122985972339",
     "grade": true,
     "grade_id": "MSELoss",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [ 0.65961179 -0.02219411  0.58831347]\n",
      "Numerical gradient:\n",
      " [ 0.65961179 -0.02219411  0.58831347]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Next we compare our implementation using numerical computations of the gradient\n",
    "def test_MSELoss_backward():\n",
    "    y = np.random.randn(3)\n",
    "    target = np.zeros(3)  # Dummy target\n",
    "    loss = MSELoss()  # Create the loss\n",
    "    loss_value = loss.forward(y, target)  # Do forward computations\n",
    "\n",
    "    dy = loss.backward()\n",
    "    print('Analytical gradient:\\n', dy)\n",
    "    dy_num = numerical_gradient(lambda y: loss.forward(y, target), y)\n",
    "    print('Numerical gradient:\\n', dy_num[0])\n",
    "    assert np.allclose(dy, dy_num), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_MSELoss_backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "2a03e143805589f6a2a38f88e7df4d76",
     "grade": false,
     "grade_id": "cell-e5a601312e669156",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "If the input `y` of the loss function is the output of a neural network, now we know how to compute the gradient of the loss wrt the network's outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "86af001ec840c368c14f6ebf7134fc15",
     "grade": false,
     "grade_id": "cell-4528f00b8cfa797b",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 2. Backpropagation through activation functions\n",
    "\n",
    "Next we propagate the gradients further (backward) through the network. Suppose that somewhere in the network, we have *element-wise* nonlinearities applied to input vector $\\mathbf{x}$ to produce outputs $\\mathbf{y}$:\n",
    "$$\n",
    "\\mathbf{y} = f(\\mathbf{x}) \\quad \\text{such that} \\quad y_i = f(x_i).\n",
    "$$\n",
    "\n",
    "When we backpropagate through that block, we need to transform the gradients $\\frac{\\partial c}{\\partial \\mathbf{y}}$ wrt to the outputs into the gradients wrt the inputs $\\frac{\\partial c}{\\partial \\mathbf{x}}$. Your task is to implement the forward and backward computations for `Tanh` nonlinearity.\n",
    "\n",
    "Notes:\n",
    "* We recommend you to compare analytical and numerical computations of the gradient.\n",
    "* If you use function `numerical_gradient` to differentiate numerically `Tanh.forward()` using a one-dimensional array `x` as input, the output of `numerical_gradient` is a two-dimensional array (Jacobian matrix). We are interested only in the diagonal elements of that array because the nonlinearity is applied *element-wise*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "fa44189b261dac9e96f5010931104389",
     "grade": false,
     "grade_id": "cell-253845a536af57eb",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class Tanh:\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (n_features,).\n",
    "        \"\"\"\n",
    "        y=np.tanh(x)\n",
    "        self.x=x\n",
    "        return y\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (n_features,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (n_features,).\n",
    "        \"\"\"\n",
    "        \n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        dx=dy/np.square(np.cosh(self.x))\n",
    "        return dx\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        #raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "074836b4d5aec585e558c90c938d11c7",
     "grade": false,
     "grade_id": "cell-87d401556813cba7",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_Tanh_shapes():\n",
    "    x = np.random.randn(3)\n",
    "    act_fn = Tanh()\n",
    "    y = act_fn.forward(x)\n",
    "    dy = np.arange(1, 4)\n",
    "    dx = act_fn.backward(dy)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_Tanh_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "88a22789b07f7adaaf1cdfb27e57a459",
     "grade": false,
     "grade_id": "cell-0ad8f605304b4bbe",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recommended you to compare analytical and numerical computations of the gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "59321566c8cf8f41f06d6f1fd78dee7e",
     "grade": true,
     "grade_id": "Tanh",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests Tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "567379ccf525c27aee1006e682edb895",
     "grade": false,
     "grade_id": "cell-54b458a07135108a",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 3. Backpropagation through a linear layer\n",
    "\n",
    "Next we propagate the gradients (backward) through a linear layer. The linear layer implements forward computations:\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}.\n",
    "$$\n",
    "\n",
    "In the backward pass, the linear layer receives the gradients wrt to the outputs $\\frac{\\partial c}{\\partial \\mathbf{y}}$ and it needs to compute:\n",
    "* the gradients wrt the layer parameters $\\mathbf{W}$ and $\\mathbf{b}$\n",
    "* the gradient $\\frac{\\partial c}{\\partial \\mathbf{x}}$ wrt the inputs.\n",
    "\n",
    "We implement the forward and the backward computations in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ea369a9db408d1abb0a31ddb3f693c91",
     "grade": false,
     "grade_id": "cell-6a69458fa99a63e4",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def linear_forward(x, W, b):\n",
    "    \"\"\"Forward computations in a linear layer:\n",
    "        y = W x + b\n",
    "\n",
    "    Args:\n",
    "      x (array): Input of shape (in_features,).\n",
    "      W (array): Weight matrix of shape (out_features, in_features).\n",
    "      b (array): Bias term of shape (out_features,).\n",
    "\n",
    "    Returns:\n",
    "      y (array): Output of shape (out_features,).\n",
    "    \"\"\"\n",
    "    return np.dot(W, x) + b\n",
    "\n",
    "def linear_backward(dy, x, W, b):\n",
    "    \"\"\"Backward computations in a linear layer.\n",
    "\n",
    "    Args:\n",
    "      dy (array): Gradient of a loss wrt outputs, shape (out_features,).\n",
    "      x (array): Input of shape (in_features,).\n",
    "      W (array): Weight matrix of shape (out_features, in_features).\n",
    "      b (array): Bias term of shape (out_features,).\n",
    "\n",
    "    Returns:\n",
    "      dx (array): Gradient wrt inputs (in_features,).\n",
    "      dW (array): Gradient wrt weight matrix W, shape (out_features, in_features).\n",
    "      db (array): Gradient wrt bias term b, shape (out_features,).\n",
    "    \"\"\"\n",
    "    dx=dy@W\n",
    "  \n",
    "    dW=np.outer(dy, x)\n",
    "    db=dy\n",
    "    \n",
    "    return dx, dW, db\n",
    "    # YOUR CODE HERE\n",
    "    # raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ca05252ce31235fdc010f12ba684f1f7",
     "grade": false,
     "grade_id": "cell-1e982ee44d097eef",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_linear_shapes():\n",
    "    x = np.random.randn(2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(1, 4)  # Gradient wrt y selected arbitrarily\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "    assert dW.shape == W.shape, f\"Bad dW.shape: {dW.shape}\"\n",
    "    assert db.shape == b.shape, f\"Bad db.shape: {db.shape}\"\n",
    "    print('Success')\n",
    "\n",
    "test_linear_shapes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c1be3aa8156732c35f162cbf8c7712a0",
     "grade": false,
     "grade_id": "cell-0b2c992d843afe2b",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Now we test the computations of $\\frac{\\partial c}{\\partial \\mathbf{W}}$ numerically.\n",
    "\n",
    "In the code below, we used the [reshaping trick](#reshaping_trick) because we want to compute derivatives\n",
    "wrt all elements of a two-dimensional input `W` of function `linear_forward`. Function `numerical_gradient`\n",
    "returns a two-dimensional array of shape `(3, 3*2)` which we combine further with the selected value of `dy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6b2af5a4ba681769a4f20f6408ef929b",
     "grade": false,
     "grade_id": "cell-f0c322ca1d46e825",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [[ 0.49043575 -0.20452946]\n",
      " [ 0.98087151 -0.40905892]\n",
      " [ 1.47130726 -0.61358838]]\n",
      "Numerical gradient:\n",
      " [[ 0.49043575 -0.20452946]\n",
      " [ 0.98087151 -0.40905892]\n",
      " [ 1.47130726 -0.61358838]]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "# Test gradient wrt W numerically\n",
    "def test_dW_numerically():\n",
    "    x = np.random.randn(2)\n",
    "    W = np.random.randn(3, 2)\n",
    "    b = np.random.randn(3)\n",
    "\n",
    "    y = linear_forward(x, W, b)\n",
    "    dy = np.arange(1, 4)  # Gradient wrt y selected arbitrarily\n",
    "    dx, dW, db = linear_backward(dy, x, W, b)\n",
    "    print('Analytical gradient:\\n', dW)\n",
    "\n",
    "    dW_num = numerical_gradient(lambda W: linear_forward(x, W.reshape(3, 2), b), W.flatten())\n",
    "    dW_num = np.dot(dy.T, dW_num).reshape(3, 2)\n",
    "    print('Numerical gradient:\\n', dW_num)\n",
    "    assert np.allclose(dW, dW_num), 'Analytical and numerical results differ.'\n",
    "    print('Success')\n",
    "\n",
    "test_dW_numerically()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "056d5c5b86027fb2a5b4617568417af6",
     "grade": false,
     "grade_id": "cell-ad8043675fc8e443",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "We recommend you to compare analytical and numerical computations of the gradients also wrt input `x` and bias term `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "28f78789acca0fc88cb3916adf6b54d0",
     "grade": true,
     "grade_id": "linear_Wb",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3028c7064b11e4110be57f8db9ab3f8b",
     "grade": true,
     "grade_id": "cell-5aa202911f55387a",
     "locked": true,
     "points": 0.25,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "a6b1419652c94f810a6cf9afee9477ef",
     "grade": true,
     "grade_id": "linear_x",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests linear_forward and linear_backward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "9320252b531d5bfe7be845eac846b432",
     "grade": false,
     "grade_id": "cell-b121dbfaf4f713ca",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Next we implement a class that represents a linear layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "d13aaeffd22937f7dd531edf0455e6e3",
     "grade": false,
     "grade_id": "cell-cf7ed19f1e61067d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "class Linear:\n",
    "    def __init__(self, in_features, out_features):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          in_features (int): Number of input features.\n",
    "          out_features (int): Number of output features.\n",
    "        \"\"\"\n",
    "        self.W = np.random.randn(out_features, in_features)\n",
    "        self.b = np.random.randn(out_features)\n",
    "\n",
    "        self.grad_W = None\n",
    "        self.grad_b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (in_features,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (out_features,).\n",
    "        \"\"\"\n",
    "        self.x = x  # Keep this for backward computations\n",
    "        return linear_forward(x, self.W, self.b)\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (out_features,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (in_features,).\n",
    "        \"\"\"\n",
    "        assert hasattr(self, 'x'), \"Need to call forward() first.\"\n",
    "        assert dy.size == self.W.shape[0]\n",
    "        dx, self.grad_W, self.grad_b = linear_backward(dy, self.x, self.W, self.b)\n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "673561841c22de3d636c652ac25ee24d",
     "grade": false,
     "grade_id": "Linear",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# We can now create a linear layer, ...\n",
    "layer = Linear(in_features=3, out_features=2)\n",
    "\n",
    "# do forward computations ...\n",
    "x = np.random.randn(3)\n",
    "y = layer.forward(x)\n",
    "\n",
    "# and backward computations\n",
    "dy = np.arange(1, 3)\n",
    "dx = layer.backward(dy)\n",
    "\n",
    "# We now have the gradients computed\n",
    "# wrt input x\n",
    "assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "# wrt weight matrix W\n",
    "assert layer.grad_W.shape == layer.W.shape, f\"Bad grad_W.shape: {layer.grad_W.shape}, W.shape={layer.W.shape}\"\n",
    "# wrt bias term b\n",
    "assert layer.grad_b.shape == layer.b.shape, f\"Bad grad_b.shape: {layer.grad_b.shape}, b.shape={layer.b.shape}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "54173c1e6db9634ada291fed6e771ba8",
     "grade": false,
     "grade_id": "cell-b236e8b574736d45",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "## 4. Define a multilayer perceptron and do full backpropagation\n",
    "\n",
    "Now let us define a multilayer perceptron and do backpropagation of the gradients from its outputs to its inputs. Your task is to implement forward and backward computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6a44b2246d47a4d313ac1239818ac09e",
     "grade": false,
     "grade_id": "cell-410dc7c2f2257317",
     "locked": false,
     "schema_version": 3,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "class MLP:\n",
    "    def __init__(self, in_features, hidden_size1, hidden_size2, out_features):\n",
    "        \"\"\"Multilayer perceptron network with two hidden layers and tanh nonlinearity in both hidden layers.\n",
    "        \n",
    "        Args:\n",
    "          in_features (int): Number of inputs.\n",
    "          hidden_size1 (int): Number of units in the first hidden layer.\n",
    "          hidden_size2 (int): Number of units in the second hidden layer.\n",
    "          out_features (int): Number of outputs.\n",
    "        \"\"\"\n",
    "        self.fc1 = Linear(in_features, hidden_size1)\n",
    "        self.activation_fn1 = Tanh()\n",
    "        self.fc2 = Linear(hidden_size1, hidden_size2)\n",
    "        self.activation_fn2 = Tanh()\n",
    "        self.fc3 = Linear(hidden_size2, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          x (array): Input of shape (in_features,).\n",
    "        \n",
    "        Returns:\n",
    "          y (array): Output of shape (out_features,).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        \n",
    "        self.x = x\n",
    "        \n",
    "        y = self.fc1.forward(x)\n",
    "        y = self.activation_fn1.forward(y)\n",
    "        y = self.fc2.forward(y)\n",
    "        y = self.activation_fn2.forward(y)\n",
    "        y = self.fc3.forward(y)\n",
    "        \n",
    "        return y\n",
    "\n",
    "    def backward(self, dy):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          dy (array): Gradient of a loss wrt outputs, shape (out_features,).\n",
    "        \n",
    "        Returns:\n",
    "          dx (array): Gradient of a loss wrt inputs, shape (in_features,).\n",
    "        \"\"\"\n",
    "        # YOUR CODE HERE\n",
    "        # raise NotImplementedError()\n",
    "        dx=self.fc3.backward(dy)\n",
    "        dx=self.activation_fn2.backward(dx)\n",
    "        dx=self.fc2.backward(dx)\n",
    "        dx=self.activation_fn1.backward(dx)\n",
    "        dx=self.fc1.backward(dx)\n",
    "        \n",
    "        return dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "bef717cc78dc290db49bfe8d8725ca61",
     "grade": false,
     "grade_id": "cell-f76c4d05c3ed8151",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytical gradient:\n",
      " [31.78842458 -8.78987605]\n",
      "Numerical gradient:\n",
      " [31.78842466 -8.78987607]\n",
      "Success\n"
     ]
    }
   ],
   "source": [
    "def test_MLP():\n",
    "    x = np.random.randn(2)\n",
    "    mlp = MLP(2, 10, 20, 5)\n",
    "    y = mlp.forward(x)\n",
    "    dy = np.arange(1, 6)  # gradient of a loss function wrt MLP's outputs\n",
    "    dx = mlp.backward(dy)\n",
    "\n",
    "    assert dx.shape == x.shape, f\"Bad dx.shape: {dx.shape}\"\n",
    "\n",
    "    # Test gradient wrt x numerically\n",
    "    print('Analytical gradient:\\n', dx)\n",
    "\n",
    "    dx_num = numerical_gradient(lambda x: mlp.forward(x), x)\n",
    "    dx_num = np.dot(dy.T, dx_num)\n",
    "    print('Numerical gradient:\\n', dx_num)\n",
    "    assert np.allclose(dx, dx_num), 'Analytical and numerical results differ'\n",
    "    print('Success')\n",
    "\n",
    "test_MLP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "cefde213839fc24fdd9453cc2385f4d4",
     "grade": true,
     "grade_id": "test_MLP",
     "locked": true,
     "points": 0.5,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# This cell tests MLP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c4b0d80c867f6854c43538f7b02e771f",
     "grade": false,
     "grade_id": "cell-37f54f36740c6999",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "Now we can connect the output of the MLP to the `MSELoss` defined previously and do full backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The gradient wrt the weights of the first layer:\n",
      " [[-0.32952488  0.84472992]\n",
      " [-1.03219875  2.64601921]\n",
      " [-1.26668651  3.24712352]\n",
      " [-1.07098929  2.74545792]\n",
      " [ 1.98285325 -5.0830015 ]\n",
      " [ 0.3235166  -0.82932782]\n",
      " [ 0.29051013 -0.74471645]\n",
      " [-0.32366491  0.829708  ]\n",
      " [ 0.36676996 -0.94020687]\n",
      " [ 0.00562815 -0.01442765]]\n"
     ]
    }
   ],
   "source": [
    "mlp = MLP(2, 10, 20, 5)\n",
    "loss = MSELoss()\n",
    "\n",
    "# Dummy inputs and targets\n",
    "x = np.random.randn(2)\n",
    "target = np.random.randn(5)\n",
    "\n",
    "# Forward computations\n",
    "y = mlp.forward(x)\n",
    "assert y.shape == target.shape, f\"Bad y.shape: {y.shape}\"\n",
    "c = loss.forward(y, target)\n",
    "\n",
    "# Backward computations\n",
    "dy = loss.backward()\n",
    "assert dy.shape == y.shape, f\"Bad dy.shape={dy.shape}\"\n",
    "mlp.backward(dy)\n",
    "\n",
    "# To update the parameters of the MLP, we can use the gradients, for example:\n",
    "print('The gradient wrt the weights of the first layer:\\n', mlp.fc1.grad_W)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0abed8a7b9f652e99ac872e4accac360",
     "grade": false,
     "grade_id": "cell-bdc01528f3086ebf",
     "locked": true,
     "schema_version": 3,
     "solution": false
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Conclusions</b>\n",
    "</div>\n",
    "\n",
    "Now you have an idea how automatic differentiation is implemented in packages like PyTorch.\n",
    "\n",
    "Note that the code above works only for propagating the error of a single data point (not a set of training examples)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc-autonumbering": false,
  "toc-showtags": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
